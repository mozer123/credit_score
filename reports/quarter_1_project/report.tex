% !TEX TS-program = xelatex
% !BIB TS-program = bibtex
\documentclass[12pt,letterpaper]{article}
\usepackage{style/dsc180reportstyle} % import dsc180reportstyle.sty
\usepackage{listings}

\lstdefinestyle{python}{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    showstringspaces=false,
    breaklines=true
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Title and Authors
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Predicting Financial Transaction Categories: A Comparative Analysis of Machine Learning Based Methods for Optimal Classification}

\author{
\begin{tabular}[t]{ccc}
Brandon Dioneda & Mert Ozer & Qianjin Zhou \\
{\tt bdioneda@ucsd.edu} & {\tt mozer@ucsd.edu} & {\tt q9zhou@ucsd.edu}
\end{tabular}
\\[4ex]
\begin{tabular}[t]{ccc}
Brian Duke (Mentor) & Kyle Nero (Mentor) & Berk Ustun (Mentor)\\
{\tt brian.duke@prismdata.com} & {\tt kyle.nero@prismdata.com} & {\tt berk@ucsd.edu}
\end{tabular}
}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Abstract and Links
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{abstract}
    \textcolor{LightGrey}{Evaluating creditworthiness has been a challenge since the 19th century, with FICO scores becoming the most widely used metric today. However, traditional credit scoring models often overlook consumers who lack conventional credit history, leading to unequal access to credit. With the advent of digital technology, alternative data sources now offer the opportunity for more inclusive credit assessments. This project addresses these gaps by analyzing non-personal financial information, such as categorizing bank transactions and predicting personal income to assess creditworthiness more fairly.}
\end{abstract}

\begin{center}
Code: \url{https://github.com/mozer123/credit_score/}
\end{center}

\maketoc
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Main Contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\subsection{Background Information}
    {Creditworthiness assessment has been a longstanding challenge, and while modern credit scores became widely adopted in 1989, these traditional models often fail to account for the financial profiles of individuals who lack conventional credit histories. As a result, millions are excluded from fair access to credit. Moreover, early credit evaluations were frequently marred by discriminatory practices, factoring in age, race, and marital status, while even today, reliance on conventional credit history can reinforce socioeconomic biases. 

    This project leverages bank data from 2017-2023 to develop a fairer, non-discriminatory model for assessing credit risk. By categorizing individual bank transactions and predicting personal income as alternative indicators, our approach emphasizes unbiased and responsible credit assessments, aiming to expand equal access to credit.
    }
\subsection{Literature Review and Discussion of Prior Work}
    {In their review, Markov, Seleznyova, and Lapshin (2022) discuss the evolving trends in credit scoring methodologies, highlighting the shift from traditional statistical methods, such as logistic regression, toward more complex machine learning models, including decision trees, neural networks, and ensemble methods. The authors emphasize the growing popularity of explainable AI (XAI) in credit scoring, as financial institutions are required to provide transparency in their decision-making processes, especially in highly regulated environments. They also underscore the importance of considering biases in model training, especially when alternative data sources are involved, as biased data can lead to discriminatory credit outcomes. This work sets the stage for understanding the inherent challenges in balancing model accuracy and fairness, particularly when advanced techniques are employed.
        
    Litty (2024) explores the integration of alternative data sources, such as social media behavior, mobile phone usage patterns, and even psychometric data, in credit risk assessment models. These sources offer the potential to improve credit scoring accuracy, especially for individuals with limited or no traditional credit history, thus addressing the issue of financial inclusion. Litty’s work outlines the effectiveness of AI-powered models in capturing complex relationships within these unstructured data sources, which are often ignored in conventional credit scoring models. Furthermore, Litty’s research identifies the potential risks of privacy invasion and data security when using personal and behavioral data, suggesting that such risks must be carefully managed to avoid ethical concerns.
    
    Together, these studies highlight the transformative role of AI in credit scoring, offering insights into both the benefits and challenges of these new methods. Unlike traditional models that rely on static financial data, our approach leverages natural language processing (NLP) on transaction memos and additional data sources to develop a more holistic view of consumer behavior. By focusing on these unstructured transaction details, we aim to extract latent features that may reveal patterns of financial responsibility not captured in traditional scoring systems. This approach not only improves the granularity of credit assessment but also offers a pathway to more personalized credit models, contributing to ongoing efforts in making credit scoring more inclusive and responsive to diverse financial behaviors.
    }
    
\subsection{Description of Relevant Data}
{Consumers' bank transactions provided by PrismData which are from years 2017-2023:}
\begin{itemize}
    \item {Inflows.pqt: Contains transaction-level information on inflowing transactions (such as: paychecks, refunds, etc.)}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{pngs/inflows.png}
        \caption{Inflows.pqt}
        \label{fig:enter-label}
    \end{figure}
    
    \item {Outflows.pqt: Contains transaction-level information on outflowing transactions (such as: groceries, rent, etc.)}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.6\textwidth]{pngs/outflows.png}
        \caption{Outflows.pqt}
        \label{fig:enter-label}
    \end{figure}
    
\end{itemize}
Columns:
\begin{itemize}
    \item {prism\_consumer\_id: ID associated with the consumer}
    \item {prism\_account\_id: ID associated with the account}
    \item {memo: Descriptive text for the transaction}
    \item {amount: Transaction amount}
    \item {posted\_date: Date when the transaction was posted}
    \item {category: Type/category of the transaction}
\end{itemize}

\section{Methods}
\subsection{Exploratory Data Analysis and Bias Checking}
\begin{enumerate}
    \item{\textbf{Summary Statistics of Transaction Amounts:}
    
    The variable (transaction) "amount” is important information that could potentially help us build our models, hence we calculated the summary statistics of “amount” for both inflow and outflow datasets.}
    \begin{itemize}
        \item {Inflow Data:}
            \begin{itemize}
                \item{Count: 513,115 transactions}
                \item{Mean amount: \$734.70}
                \item{Median amount: \$100.00}
                \item{Standard deviation: \$5,296.57}
                \item{Range: \$0.01 to \$1,154,966.00}
            \end{itemize}
    
        \item {Outflow Data:}
            \begin{itemize}
                \item{Count: 2,597,488 transactions}
                \item{Mean amount: \$145.13}
                \item{Median amount: \$24.26}
                \item{Standard deviation: \$1,697.88}
                \item{Range: \$0.00 to \$654,853.20}
            \end{itemize}
    \end{itemize}
{As shown in the summary statistics above,  inflows tend to have higher transaction amounts compared to outflows, with a significant difference in mean and maximum values.}

    \item{\textbf{Category Distribution:}}
    \begin{figure}[H]
        \centering
        \includegraphics[width=1.0\textwidth]{pngs/category_dist.png}
        \caption{Category Distribution}
        \label{fig:enter-label}
    \end{figure}
    \begin{itemize}
        \item Inflow Data: Most common categories are EXTERNAL\_TRANSFER $(30.5\%)$ and SELF\_TRANSFER $(21.5\%)$.
        \item Outflow Data: Most common categories are GENERAL\_MERCHANDISE $(20.2\%)$, FOOD\_AND\_BEVERAGES $(18.6\%)$, and EXTERNAL\_TRANSFER $(12.4\%)$.
    \end{itemize}

    {We also computed the most common merchants(“memo”) per category for the outflow dataset. The results for the inflow dataset are not displayed because all memos exactly match their categories.}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.4\textwidth]{pngs/cmn_merchant.png}
        \caption{Most Common Merchant per Category - Outflow Dataset}
        \label{fig:enter-label}
    \end{figure}
    {From Figure 4 we can see that McDonald’s, Amazon, Walmart, Uber, etc. are the most common merchants for each category they belong to. It is worth noting that the process of finding the most common merchant is based on the uncleaned “memo” column.}
\end{enumerate}

\subsection{Data Cleaning}
{To prepare our dataset for further analysis and modeling, we cleaned the memo column.}
{Here is how we cleaned the memo with considerations we made along the way:}

\begin{itemize}
    \item Filtered rows where the memo field differs from the category field, which would allow our future models to look for more meaningful features 
    \item The resulting unique categories are: ['FOOD\_AND\_BEVERAGES', 'GENERAL\_MERCHANDISE', 'GROCERIES', 'PETS', 'TRAVEL', 'MORTGAGE', 'OVERDRAFT', 'EDUCATION', 'RENT']
    \item Converted all bank memos to lowercase
    \item Removed special characters and numbers to focus on text-based features
    \item Removed placeholders (e.g., sequences like "xxx")
    \item Trimmed extra spaces to clean up the text field further
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{pngs/memo_clean.png}
    \caption{Original Memos vs Cleaned Memos}
    \label{fig:enter-label}
\end{figure}

\subsection{Feature Engineering}
\begin{itemize}
    \item {Features based on Memo:}
        \begin{itemize}
            \item {TF-IDF}
        \end{itemize}
    \item {Features based on Date:}
        \begin{itemize}
            \item 
                \texttt{date\_month}: Indicates the month that a transaction was processed.            
            \item 
                \texttt{date\_day}: Indicates the weekday that a transaction was processed. 
                
            \item 
                \texttt{date\_weekend}: Indicates if a transaction was processed on a weekend. Certain transactions are never processed over Saturday/Sunday.
        \end{itemize}
    \item {Features based on Amount:}
        \begin{itemize}
            \item 
                \texttt{is\_whole\_num:}: Indicates whether a transaction amount is a whole number. This can provide insight into the nature of the transaction—for example, most food and beverage transactions typically have fractional amounts, whereas rent payments are often whole numbers. 
        \end{itemize}
\end{itemize}

\subsection{Models for Categorization}
These are the various methods we tried using as a way to categorize an individual's bank transactions. We believe that being able to accurately categorize one’s bank transactions helps us gain insight into people’s financial behavior. How do they spend their money? What do they spend it on? By answering these questions, we aim to build a clearer picture of an individual’s financial stability and habits, ultimately enabling a more reliable assessment of their creditworthiness.
\paragraph{Traditional Classifiers:}
    \begin{enumerate}
        \item {Logistic Regression as a Baseline}
            \begin{itemize}
                \item {Logistic Regression was used as a starting point for comparison, since it is easily to implement and prediction time is relatively fast}
                \item {Simply used default parameters from sklearn Logistic Regression that only used TF-IDF on the memo column for features}
            \end{itemize}
        \item {FastText}
            \begin{itemize}
                \item {FastText is a lightweight and efficient text classification library by Facebook. It treats text data as a bag of words or bag of n-grams, providing a simple but effective approach for capturing word relationships and context}
                \item {We processed the data to align with the format required by the FastText model and trained the model with hyperparameters (epoch=30, lr=0.1, wordNgrams=2) optimized through grid-search and cross-validation}
            \end{itemize}
        \item {XGBoost}
            \begin{itemize}
                \item{We also attempted our predictive task with the gradient-boosted decision tree model from the \textit{xgboost} library. XGBoost excels in structured data tasks. It captures complex feature interactions and is optimized for speed and accuracy. The XGBoost classifier was trained using TF-IDF features derived from the memo text data.}
            \end{itemize}
    \end{enumerate}

\paragraph{Large Language Models (LLMs):}

\begin{enumerate}
        \item {DistilBERT}
            \begin{itemize}
                \item {DistilBERT is a lightweight transformer model derived from BERT, designed to retain most of BERT’s accuracy while being faster and more efficient. It utilizes deep contextual embeddings to understand semantic relationships in text, making it effective for text classification tasks.}
                \item {The dataset was reduced to 12,500 rows due to computational resource constraints. DistilBERT was fine-tuned for the task using the Hugging Face library, with hyperparameters including 3 epochs of training. The model leveraged pre-trained weights and adapted them to the transaction classification task.}
            \end{itemize}
        \item {Nemotron 70Bl}
            \begin{itemize}
                \item {We utilized the open-source model Nemotron 70B from NVIDIA. Each prompt included five examples demonstrating the desired response format and predefined category options. Inference was conducted via an API service rather than being hosted locally.}
            \end{itemize}
        \item {Llama-3.2-1B}
            \begin{itemize}
                \item{We employed an instructable version of the open-source model Llama-3.2-1B from Meta for this task. To optimize the fine-tuning process, we leveraged Unsloth, a library designed to enhance processing speed and reduce memory consumption. The fine-tuning was conducted in the Google Colab environment.}
            \end{itemize}
    \end{enumerate}



\subsection{Income}


\section{Results}


\begin{table}[h!]
\centering
\begin{tabular}{|l|p{2.5cm}|p{1.7cm}|p{3.5cm}|p{2.5cm}|p{1.7cm}|}
\hline
\textbf{Model}           & \textbf{Training Dataset Size} & \textbf{Training Time (s)} & \textbf{Inference (predictions per second)} & \textbf{Testing Dataset Size} & \textbf{Accuracy (\%)} \\ \hline
Logistic Regression      & 979,839                        & 32.8                       & 326,613                                     & 326,613                      & 96.45                 \\ \hline
FastText                 & 969,666                        & \textbf{19.1}                       & \textbf{336,786}                                     & 336,786                      & \textbf{96.88}                 \\ \hline
XGBoost                  & 969,666                        & 30.9                       & 300,000                                     & 336,786                      & 91.98                 \\ \hline
DistilBERT               & 12,500                         & 600                       & 54.67                                       & 12,500                       & 89.56                 \\ \hline
Nemotron 70B             & 5                              & N/A                        & 0.44                                        & 100                          & 78.00                    \\ \hline
Llama-3.2-1 B            & 1,000                          & 72                         & 3.04                                        & 100                          & 75.00                    \\ \hline
\end{tabular}
\caption{Performance comparison of models on transaction categorization.}
\label{tab:model_comparison}
\end{table}





\section{Discussion}
\subsection{Traditional Classifiers}

The performance comparison between Logistic Regression, FastText, and XGBoost reveals interesting trade-offs among accuracy, training time, and inference speed. FastText achieved the highest accuracy at 96.88\%, outperforming both Logistic Regression (96.45\%) and XGBoost (91.98\%).

FastText had the fastest training time at 19.1 seconds, making it highly efficient for large-scale datasets like this one. Its inference speed was also the fastest, processing approximately 336,786 predictions per second.

XGBoost lagged behind in accuracy at 91.98\% and had a slower inference speed of 300,000 predictions per second, despite a reasonable training time of 30.9 seconds.

Overall, FastText stands out as the best-performing model in terms of accuracy, speed, and scalability, making it an excellent choice for text-based transaction classification tasks.


\subsection{Thoughts on LLMs}

Ideally, we aim for a local solution that offers high accuracy and fast inference while minimizing computational resource requirements. Although longer training times can be a drawback during testing, this is more acceptable compared to other factors, as the few-shot training premodel will only need to be trained once in the production phase.

 While inference speed may not be optimal in our scenario, hosting the model locally on a supercomputer can significantly improve processing capabilities. However, accuracy remains a concern, as the LLM often generates generic responses based on its pre-trained knowledge. For instance, we observed it predicting "GENERAL\_MERCHANDISE" more frequently than appropriate.
 
Fine-tuning the LLMs could address the accuracy issues associated with few-shot training. However, achieving higher accuracy has been difficult due to the substantial computational resources required for fine-tuning. Despite this, the approach demonstrates promise: even when we utilized a much smaller model, it performed comparably to a larger model without fine-tuning.


\section{Conclusion}

Our analysis demonstrates that FastText is the most effective solution for transaction categorization, excelling in accuracy (96.88\%), speed, and scalability. Logistic Regression provided a reliable baseline, while XGBoost, despite its utility in structured data tasks, underperformed in this context. Large Language Models showed potential but were limited by high computational costs and slower inference speeds. Overall, FastText offers the best balance of performance and efficiency for this task, while future improvements in LLM fine-tuning may further enhance their applicability.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Reference / Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\makereference

{\color{blue} To edit the contents of the ``References" section, edit \texttt{reference.bib}. Many conference websites format citations in BibTeX that you can copy into \texttt{reference.bib} directly; you can also search for the paper on Google Scholar, click ``Cite", and then click ``BibTeX" (\href{https://scholar.google.com/scholar?hl=en&as_sdt=0%2C23&q=attention+is+all+you+need&btnG=#d=gs_cit&t=1700436667623&u=%2Fscholar%3Fq%3Dinfo%3A5Gohgn6QFikJ%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Den}{here}'s an example).}

\bibliography{reference}
\bibliographystyle{style/dsc180bibstyle}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\makeappendix

\subsection{Training Details}

\subsection{Additional Figures}

\subsection{Additional Tables}


\end{document}