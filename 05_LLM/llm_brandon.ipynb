{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0833f02a-1f94-4928-b8a8-1eca78bfe421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 21:25:06.196150: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-07 21:25:06.196190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-07 21:25:06.197742: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-07 21:25:06.206961: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1a2256-23fc-4e80-bef1-508a4edb8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outflows = pd.read_parquet('outflows.pqt')\n",
    "outflows.posted_date = pd.to_datetime(outflows.posted_date, format='%Y-%m-%d')\n",
    "\n",
    "categories = ['GENERAL_MERCHANDISE', 'FOOD_AND_BEVERAGES', 'GROCERIES', 'TRAVEL', 'PETS', 'MORTGAGE', 'OVERDRAFT', 'EDUCATION', 'RENT']\n",
    "outflows = outflows[outflows.category.isin(categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ac597c-cf42-4f8b-a78a-608bcce9f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "outflows['new_memo'] = outflows.memo.str.lower().str.replace(r'\\*(\\S)', r' \\1', regex=True)     # Remove all '*' but gotta check if there's any alphanumerics right after it\n",
    "outflows['new_memo'] = outflows.new_memo.str.replace(r'[^a-z0-9]', ' ', regex=True)             # Remove all special chars\n",
    "outflows['new_memo'] = outflows.new_memo.str.replace(r'x{3,}', ' ', regex=True)                 # Remove all triple X's\n",
    "outflows['new_memo'] = outflows.new_memo.str.replace(r'withdrawal', 'withdrawal ', regex=True)  # Fixing the withdrawal rows\n",
    "outflows['new_memo'] = outflows.new_memo.str.replace(r'\\s+', ' ', regex=True).str.strip()       # Remove any extra whitespace\n",
    "\n",
    "outflows = outflows[['prism_consumer_id', 'prism_account_id', 'memo', 'new_memo', 'amount',\t'posted_date', 'category']]\n",
    "# outflows[['memo', 'new_memo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed6b43f5-fd70-49d0-bfa7-90647254122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outflows.drop(columns='memo', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad775c-d6db-4704-8edd-3db780313a6c",
   "metadata": {},
   "source": [
    "## LLM:\n",
    "    - RoBERTa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44f3f90a-8bf2-4a12-aeaa-4e4f53924e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset = outflows.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be78c91-a9b2-4e00-8aa5-1576582b56fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(922256, 384196)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42069)\n",
    "\n",
    "# Splitting based on prism_consumer_id:\n",
    "cids = outflows.prism_consumer_id.unique()\n",
    "train_cids, test_cids = train_test_split(cids, test_size=0.3, random_state=420) # 70 / 30 split\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "outflows['category_encoded'] = label_encoder.fit_transform(outflows.category)\n",
    "\n",
    "# Features and labels for train and test sets:\n",
    "X_train = outflows[outflows.prism_consumer_id.isin(train_cids)]\n",
    "X_test  = outflows[outflows.prism_consumer_id.isin(test_cids)]\n",
    "\n",
    "y_train = X_train.category_encoded\n",
    "y_test  = X_test.category_encoded\n",
    "\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dabbbfa6-ed3c-49b4-93bc-973e0297c99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a4392bc2264960ac625d7ccdccf3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6601a9efaffa403f85d538eca9c40577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d067e65756e74283970ef7c565a206cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6ae6ea439c4e9ca605d2199d02dcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1752f314dc4d92b93b3151d164a675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ede085141244b1833565ca0a9f4e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model     = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb11d0a8-19a4-4839-ba86-4514933a7d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa5f5d83-b497-49af-bda6-a787c3cb86a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389005e77a2e4fb6a93db3d7f6c563b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/922256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851aca973e304f04b60e39bbcd855fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/384196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = Dataset.from_dict({\"text\": X_train.new_memo, \"label\": y_train})\n",
    "test_data = Dataset.from_dict({\"text\": X_test.new_memo, \"label\": y_test})\n",
    "\n",
    "train_data = train_data.map(tokenize_data, batched=True)\n",
    "test_data = test_data.map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862c2c2-e38a-4efc-9923-17cb6858ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",        # Still evaluate every epoch, if desired\n",
    "    save_strategy=\"no\",           # Disable checkpoint saving\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9447b1f8-11d0-4c15-b906-5b729fe56cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48025' max='48025' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48025/48025 2:23:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.193136215209961,\n",
       " 'eval_model_preparation_time': 0.0029,\n",
       " 'eval_accuracy': 0.34681256442024383,\n",
       " 'eval_f1': 0.1942345353899732,\n",
       " 'eval_precision': 0.13489036077377844,\n",
       " 'eval_recall': 0.34681256442024383,\n",
       " 'eval_runtime': 8616.9834,\n",
       " 'eval_samples_per_second': 44.586,\n",
       " 'eval_steps_per_second': 5.573}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    precision = precision_score(labels, predictions, average=\"weighted\")\n",
    "    recall = recall_score(labels, predictions, average=\"weighted\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.compute_metrics = compute_metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a85d11-313f-4435-adc3-9a6c66916eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
